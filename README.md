# Projects Description:

This project is a classification problem, aiming to the developement of a model capable of differentiating fake job postings from real ones. In order to achieve this goal, the big data processing engine Apache Spark was used on top of python implementation language. The main PySpark library used was Spark NLP. The models developed are required to use only the description collumn from the job postings csv file for training.

## Instructions for installation:

John Snow Labs provide very detailed instructions for installation on the following link:
https://nlp.johnsnowlabs.com/docs/en/install

![Screenshot](https://www.hhmglobal.com/wp-content/uploads/techno-trends/28422/johnsnowlabs_logo.jpg)


## Implementation using Google Collab:

! pip install -q pyspark==3.2.0 spark-nlp==3.4.2. # basic requirements (or any other compatible pyspark and sparknlp versions). Other libraries are 
                                                  # included on the code sections.


## Setup used:

Google Collab 

![Screenshot](https://www.hwlibre.com/wp-content/uploads/2021/11/google-colaboratory.jpg)

## Pretrained Classifiers used:

- sent_small_bert_L8_128   (Bert Sentence Embedder)
-  Word Embeddings Classifierdl Pretrained


Some conclusions can be seen in the presentation file!

## Contact:

Feel free to contact me on vagelis451@gmail.com

